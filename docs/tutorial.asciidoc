= Just Annotate My Genome (JAMg) tutorial
:Author:    Alexie Papanicolaou
:Email:     alexie@butterflybase.org
:Date:      December 2013
:Revision:  RC1

A tutorial for Just_Annotate_My_genome using 'Drosophila melanogaster', a small amount (50 million pairs) of RNA-Seq and public Sanger sequences.

In modern projects you will have substantially more RNA-Seq (at least 150 million, i.e. one HiSeq lane) and probably little to no Sanger/454 sequences. However, this makes the tutorial shorter and shows how even established genomes can be annotated (it is also a shameless plug for http://insectacentral.org[InsectaCentral].

This tutorial assumes you know how to use a (ba)sh Linux shell, can create and navigate through directories. In the tutorial we don't create any special directories, but in real-world scenarios you would. You'd also keep a log of what you've done (and why), keeping track if something didn't work. If it was a bug from our side, please let us know (but first ask your system administrator if you don't know how to use Linux too well). I also assume you know not to overload your server(s) in terms of CPUs and I/O (hard disk) or network bandwidth. Use 'top' to see if you caused such a bottleneck.

Further, in this tutorial we are skipping a step: aligning 'known proteins' to the genome. Because our tutorial uses 'Drosophila melanogaster', if we did that then we would be essentially using the official annotations so we wouldn't be able to 'validate' the procedure. In a real-world scenario, you would follow the link:procedure.html#foreign_proteins[procedure] to align known proteins of your choice. More tips on using JAMg with real-world scenario will be scattered through out this tutorial.

TIP: At times, entries like this will appear. They offer commentary, tips or warnings. For example, many of the commands that you will run below may take hours. Considering that you will be using a server with SSH, it is advisable that you use the unix command 'screen' to ensure that an intermittent connection error doesn't cramp your style.

Steps that share a number can be run in parallel:

== Step 0: Configure and 'download' example data
I'm assuming you've downloaded the software from somewhere and unpacked it. The next step is to store the path in an environmental variable so you can copy-paste for the rest of the tutorial. Also we will export any other parameters that are species specific and we will use for the rest of the tutorial. You will also need to run 'make' which make take some time (unzipping files etc) but it ought to be un-attended (any errors will cause it to crash).

Basic installation instructions:

[source,bash]
cd $JAMG_PATH
make # will install the various software, including RepeatMasker
# Go to www.giriinst.org and install the repeat library in $JAMG_PATH/3rd_party/RepeatMasker
# Run configure. Use the RMBlast NCBI option when asked about an engine (the engine is in $JAMG_PATH/3rd_party/RepeatMasker/ncbi-blast)
$JAMG_PATH/3rd_party/RepeatMasker/configure

Then download the refseq_insecta_march13_just_useful.tar.bz2 HHblits database from link:https://sourceforge.net/projects/jamg/files/databases/[here] and uncompress it in $JAMG_PATH/databases/hhblits

Finally, we also assume you have correctly installed PASA (and mySQL) to work with your computing environment. Prepare the configuration file

[source,bash]
cp $JAMG_PATH/3rd_party/PASA/pasa_conf/pasa.CONFIG.template $JAMG_PATH/3rd_party/PASA/pasa_conf/conf.txt
# Edit conf.txt and set the values for these MySQL database settings:
# MYSQLSERVER=(your mysql server name)
# MYSQL_RO_USER=(mysql read-only username)
# MYSQL_RO_PASSWORD=(mysql read-only password) - leave empty if none used
# MYSQL_RW_USER=(mysql all privileges username)
# MYSQL_RW_PASSWORD=(mysql all privileges password) - leave empty if none used
# PASA_ADMIN_DB=pasa_admin # we need something here but it is not used
# you do not need to worry about the others 
vim $JAMG_PATH/3rd_party/PASA/pasa_conf/conf.txt

This is all you need to do to setup PASA these days. 

Next let us prepare for the Tutorial (i.e. tutorial-specific instructions follow). First, setup some variables:

[source,bash]
export JAMG_PATH=$HOME/software/jamg # or wherever you installed it.
export MAX_INTRON_LENGTH=70000 # maximum size of intron.
export GENOME_PATH=[full path to]/dmel-all-r5.53.fasta
export GENOME_NAME=dmel-all-r5.53 # just a base name for our genome
# number of CPUs to use for the local machine. Always at least one less than the number of available CPUs:
export LOCAL_CPUS=5 
export MAX_MEMORY_G=100 # maximum amount of memory to use in Gb

These ought to be project specific so if you store the above in a file you can use this command to import them in your environment. That way you don't have to type them every time and you will also remember what you used.

[source,bash]
vim env_vash.sh # add the above data
 . ./env_vash.sh # that is a dot in the beginning

Next preparing some data. First of all, since this is a tutorial and we are analyzing on of the better assembled/annotated genomes out there ('Drosophila melanogaster', the fruitfly), we want to be able to evaluate if what we do is correct. For that we will need the official FlyBase annotations. Sadly the GFF3 provided by FlyBase is often corrupt (e.g. CDS lines appear before a gene etc) so we have post-processed their GFF, cleaned it and provide a GTF that will be used for our evaluation. It sits in the test_suite directory so:

[source,bash]
tar -xf $JAMG_PATH/test_suite/Drosophila_official_annotations_cleaned.tar melanogaster/dmel-all-no-analysis-r5.53.gff3.gff3.clean.gtf.bz2
bunzip dmel-all-no-analysis-r5.53.gff3.gff3.clean.gtf.bz2 # we will use this later during evaluation.

Now we need some input data. JAMg is specifically built for RNA-Seq type of data (for example Illumina, but it doesn't need to be NGS data, could be Sanger too - just not cost-effective to produce them anymore). How we can get some RNA-Seq data? Well we can download them from NCBI and run the following script to pre-process them:

[source,bash]
mkdir RNAseq; cd RNAseq
# we have installed Aspera and the sra_sdk from NCBI
ascp \
 anonftp@ftp-trace.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByRun/sra/SRR/SRR023/SRR023199 \
 anonftp@ftp-trace.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByRun/sra/SRR/SRR023/SRR023502 \
 anonftp@ftp-trace.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByRun/sra/SRR/SRR023/SRR023504 \
 anonftp@ftp-trace.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByRun/sra/SRR/SRR023/SRR023538 \
 anonftp@ftp-trace.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByRun/sra/SRR/SRR023/SRR023539 \
 anonftp@ftp-trace.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByRun/sra/SRR/SRR023/SRR023540 \
 .
find . -name "*sra" -exec fastq-dump --split-spot --split-files --skip-technical -F -Q 33 -W -T -R pass '{}' \;
find . -name "*sra" -delete # delete primary data if you don't need the backup
# quality pre-process:
find . -name "pass" -type d -exec $JAMG_PATH/3rd_party/preprocess_reads/preprocess_illumina.pl \
 -cdna -no_screen -paired '{}/1/fastq' '{}/2/fastq' \;
# let's see what we produced:
ls -l SRR*/pass/1/fastq.trimmomatic.bz2 # All 'left' files of a pair
ls -l SRR*/pass/2/fastq.trimmomatic.bz2 # 'right' files of a pair
ls -l SRR*/pass/?/fastq.unpaired.bz2 # remnant unpaired files when the pair has been discarded
# let's rename them to something easier to use.
$JAMG_PATH/3rd_party/preprocess_reads/rename_SRA_sra_fastq.pl
ls -l *.trimmomatic.bz2
bunzip2 -k *.trimmomatic.bz2 # highly recommend pbzip2 -d

TIP: if you want to use the traditional Trinity RNASeq way of creating input files, you can do this (but we will not use it for this tutorial):
[source,bash]
pbzip2 -dck SRR*/pass/1/fastq.trimmomatic.bz2 > drosoph_ALL.Left.fastq
pbzip2 -dck SRR*/pass/2/fastq.trimmomatic.bz2 > drosoph_ALL.Right.fastq
pbzip2 -dck SRR*/pass/?/fastq.unpaired.bz2 >> drosoph_ALL.Left.fastq # unpaired remnants can go to any file

TIP: We can also use 454 and Sanger data! Then go to http://insectacentral.org/genes4all/download/request[InsectaCentral] and download a FASTA of all 'Drosophila melanogaster' (Diptera:Drosophilidae) contigs. This is a MIRA assembly of all Sanger and 454 data that was available for D. melanogaster at the time. For the tutorial we'll assume you saved them as 'D_melanogaster_ICcontigs.fsa'.

TIP: If you want to increase the amount of data for this tutorial (and i.e are willing to wait much longer, then you can download these data which were used for testing JAMg). This will increase TGG's run time by 4 days and TDN by about half a day.
[source,bash]
ascp \
anonftp@ftp-trace.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByRun/sra/SRR/SRR767/SRR767611 \
 anonftp@ftp-trace.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByRun/sra/SRR/SRR767/SRR767619 \
 anonftp@ftp-trace.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByRun/sra/SRR/SRR767/SRR767621 \
 anonftp@ftp-trace.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByRun/sra/SRR/SRR767/SRR767623 \
.

Next we would like to get the Drosophila genome and official annotations in order to evaluate our performance. In a real life scenario this is not possible as you are the ones building the annotation from scratch! However, you can use this procedure to compare different types of annotations (as well as see how JAMg performs). This tutorial was built with FlyBase version 5.53. One can download the GFF from FlyBase (the file with 'no-analysis') but we found that the melanogaster file had a number of errors (e.g. CDS before the parent genes or CDSs without any exon and in one instance a weakly supported gene model that we couldn't translate with GTF etc). We've cleaned it up and made the GTF we will use for evaluation.

ls $JAMG_PATH/test_suite/Drosophila_official_annotations_cleaned.tar
tar -xf $JAMG_PATH/test_suite/Drosophila_official_annotations_cleaned.tar
bunzip2 -v $JAMG_PATH/test_suite/*bz2
ls -l dmel-X-r5.53.fasta dmel-X-r5.53.repeat.hard dmel-X-r5.53.repeat.soft melanogaster/

== Step 1a: RepeatMask and 'identify untranscribed coding exons'
In this step you will check if there are any domains that are coding. This will provide evidence even in the absence
of RNA-Seq data. The script will also run RepeatMasker the output of which we will use later on. 

[source,bash]
mkdir exon_search; cd exon_search
# MPI with many hosts; localhost (morgan) uses 5 threads for repeatmasker
$JAMG_PATH/bin/prepare_domain_exon_annotation.pl -verbose -genome $GENOME_PATH \
 -repthreads $LOCAL_CPUS -engine mpi -hosts morgan:5-haldane3:12-haldane2:10-haldane1:5-haldane4:12 -mpi 44 \
 -uniprot_db $JAMG_PATH/databases/hhblits/refseq_insecta_march13_just_useful \
 -scratch /dev/shm/$USER
# OR  MPI with a single local host and 5 CPUs
$JAMG_PATH/bin/prepare_domain_exon_annotation.pl -verbose -genome $GENOME_PATH \
 -repthreads $LOCAL_CPUS -engine localmpi -mpi $LOCAL_CPUS \
 -uniprot_db $JAMG_PATH/databases/hhblits/refseq_insecta_march13_just_useful \
 -scratch /dev/shm/$USER
ls ./*hints # should be two files
# RepeatMasker is going to be run above. Once finished, run this as later we will need a "soft-masked" genome:
$JAMG_PATH/3rd_party/bin/maskFastaFromBed -soft -fi $GENOME_PATH -fo $GENOME_PATH.softmasked \
 -bed $GENOME_PATH.out.gff # this last file is the output from RepeatMasker

Purely FYI: in my local cluster environment using MPI with 44 CPUs and the databases copied to /dev/shm (first command above), this step took 36h (36h and 17 minutes to be exact). The network speed used for MPI was a bottleneck (we just have 10gb Ethernet not infiniband).

Now apply the repeatmasking output file $GENOME_PATH.out.gff to create a 'soft'masked file that we will use later on:

[source,bash]
maskFastaFromBed -soft -fi $GENOME_PATH -fo $GENOME_PATH.softmasked -bed $GENOME_PATH.out.gff

== Step 1b: Assembly transcriptome to create 'high-quality gene models'
While the previous step is running, prepare TDN and TGG assemblies (Trinity de-novo and Trinity genome-guided) for the RNA-seq data.

[source,bash]
mkdir Trinity_assemblies; cd Trinity_assemblies
$JAMG_PATH/3rd_party/trinityrnaseq/Trinity.pl --seqType fq --min_kmer_cov 2 \
 --left ../*1_fastq.trimmomatic ../*_unpaired_fastq.trimmomatic --right ../*_2_fastq.trimmomatic \
 --output TDN --JM "$MAX_MEMORY_G"G --CPU $LOCAL_CPUS --full_cleanup |& tee tdn.log # expected output is a Trinity.fasta
# the above will take some time. First step is unzipping all the input files before JellyFish start running
# In parallel prepare for TGG. First align the reads to the genome:
mkdir TGG; cd TGG
$JAMG_PATH/bin/align_rnaseq_gsnap.pl -fasta $GENOME_PATH -dbname $GENOME_NAME -cpus $LOCAL_CPUS \
 -nofail -suffix -input_dir ../../ |& tee tgg.log

In the above we have prepared the input for TDN and TGG (Trinity de-novo and Trinity Genome-guided). For this tutorial, for TDN we use '--min_kmer_cov 2' because it saves time and resources but in a real world scenario of annotating a genome, don't use it if you don't have to. For TGG we use '-suffix' because we don't expect any substantial polymorphism for Drosophila melanogaster. If you have a species with polymorphism then don't use '-suffix' (actually in my tests using -suffix made the search slower rather than faster, the opposite of what is expected). Once the alignment of the RNASeq is complete we can continue with the TGG process. Some files will be mapped multiple times. We know that Drosophila is well assembled so these RNASeq are almost certainly repeats, for this tutorial we will not use them. The overall process above will take about 16-24h if you're doing both in parallel and using 5 CPUs for TDN and 10 CPUs for TGG.

CAUTION: In NGS-derived assemblies, it is not uncommon to have 'haplotype' scaffolds (see the Heliconius genome paper), for that reason we would keep them but decrease the '-path_number' option of 'align_rnaseq_gsnap' from the default of 50 to something that is expected for your assembly (e.g. 4). See the files TGG/*.concordant_mult_xs for read pairs that map to higher than -path_number paths.

TIP: For parallelization with computing clusters, you can use the -commands_only option and create a text file that has one line worth of commands for each input. You can then use the unix command 'split' or ParaFly to run it on a cluster. I find that this GSNAP step is the 'slowest' in the entire procedure (1 day per 50 million pairs). By splitting the sequence files, we can leverage HPC architectures to get through the data in a day or so.

[source,bash]
# grab all the outputs for Drosophila
cd TGG
# prepare files for TGN, splitting them to those that will take a very long time/resources, medium and very short
$JAMG_PATH/bin/prepare_trinity_genome_assembly_pbs.pl -files ./*.concordant_uniq.bam -intron $MAX_INTRON_LENGTH
ls ./*.cmds # what needs to be run. 
# We can start assemblying the TGG data:
# In our our tutorial, only small_trinity_GG.cmds will be created. Run it.
ParaFly -CPU $LOCAL_CPUS -v -c small_trinity_GG.cmds -failed_cmds small_trinity_GG.cmds.failed
# Instead, you can use the small_trinity_GG.cmds.000 and small_trinity_GG.cmds.001
# and load them on two separate machines (or cluster). These were produced using the unix command split

CAUTION: Even though TGG is very fast, Trinity itself is rather I/O (hard disk/network read/write) demanding, especially when you are running multiple Trinity runs in parallel. Decreasing the number of CPUs / parallel runs, may complete faster (use the unix command 'top' to see if many of your commands get stuck in 'D' (delay) mode instead of 'R' (run).

NOTE: If you had used -files ./*_uniq_mult.bam rather than ./*.concordant_uniq.bam, then a 'medium_trinity_GG.cmds' would have been created too. Because Drosophila is well assembled and the reads are high-quality, these are likely to be repeats. Also, generally and very rarely, 'large_trinity_GG.cmds' may exist, especially from very large RNASeq projects. They are probably repeats or very highly expressed genes. They can take days to complete and their value is debatable. I recommend you use Trinity's kmer data reduction algorithm. Currently this has to be done manually.

While this ParaFly procedure is running, we can post-process the alignments to create RNA-Seq coverage data for Augustus (it will take considerable time):

[source,bash]
# RNASeq_TGG_input.bam is from prepare_trinity_genome_assembly_pbs.pl above
$JAMG_PATH/bin/augustus_RNAseq_hints.pl -bam RNASeq_TGG_input.bam -genome $GENOME_PATH 
 
Once TGG and TDN are complete, we can integrate TGG and TDN using http://pasa.sourceforge.net/[PASA2].

[source,bash]
find TGG/Dir_* -name "*inity.fasta" | $JAMG_PATH/3rd_party/trinityrnaseq/util/GG_trinity_accession_incrementer.pl > Trinity_GG.fasta
#p.s With Trinity_GG.fasta in your base directory, you can safely delete the TGG directory now
cat TDN/Trinity.fasta Trinity_GG.fasta > transcripts.fasta
cat TDN/Trinity.fasta | $JAMG_PATH/3rd_party/PASA/misc_utilities/accession_extractor.pl > tdn.accs
# prepare a PASA assembly configuration (separate from the PASA-wide configuration you did in the beginning)
cp $JAMG_PATH/3rd_party/PASA/pasa_conf/pasa.alignAssembly.Template.txt alignAssembly.config
# Edit the alignAssembly.config and give the database a unique name, set the following:
# MYSQLDB=jamg_drosie_tutorial
$JAMG_PATH/3rd_party/bin/seqclean transcripts.fasta -c $LOCAL_CPUS -n 10000 
# first use -x to check everything is OK create a list of commands that will be run with PASA:
$JAMG_PATH/3rd_party/PASA/scripts/Launch_PASA_pipeline.pl -c alignAssembly.config -C -R \
 -g $GENOME_PATH --MAX_INTRON_LENGTH $MAX_INTRON_LENGTH \
 --ALIGNERS blat,gmap --TRANSDECODER --CPU $LOCAL_CPUS \
 -T -t transcripts.fasta.clean -u transcripts.fasta \
 --TDN tdn.accs -x > pasa.alignAssembly.commands.to.run
# Now run it.
$JAMG_PATH/3rd_party/PASA/scripts/Launch_PASA_pipeline.pl -c alignAssembly.config -C -R \
 -g $GENOME_PATH --MAX_INTRON_LENGTH $max_intron_length \
 --ALIGNERS blat,gmap --TRANSDECODER --CPU $LOCAL_CPUS \
 -T -t transcripts.fasta.clean -u transcripts.fasta \
 --TDN tdn.accs |& tee pasa.log
ls jamg_drosie_tutorial.assemblies.fasta jamg_drosie_tutorial.pasa_assemblies*
# Find transcripts that did not make it to the genome
$JAMG_PATH/3rd_party/PASA/scripts/build_comprehensive_transcriptome.dbi -c alignAssembly.config -t transcripts.fasta.clean


This will take some time, about one day with the 2 alignment steps ('blat' and 'gmap') taking about 5-7 hours. In very large data or mission critical scenarios, we can run the alignment steps separately on a cluster and use the '-s' and '-e' options to determine which steps shown in 'pasa.alignAssembly.commands.to.run' will be run on which computer or cluster. The 'tee' command will print the screen output to a file and '|&' will copy any errors to the same file too.
 
TIP: If you've never ran PASA on this system before, it makes sense to try the first command produced in 'pasa.alignAssembly.commands.to.run'. It attempt to connect and create the database. If it doesn't work then your mySQL settings are wrong. Common errors are 'Access denied for user user_demo@localhost' when the user already exists. Giving the relevant priviliges can solve it: 
[source,bash]
mysql -u root
CREATE USER 'user_demo'@'localhost' IDENTIFIED BY 'pass13';
# OR for without a password, skip the IDENTIFIED BY part.
GRANT SELECT,INSERT,UPDATE,DELETE ON *.* TO 'user_demo'@'localhost';

== Step 1c: run de-novo predictors that require no training
There are some predictors that use no training at all. 

GeneMarkES is one such example:

[source,bash]
$HOME/software/genemark/gm_es_bp_linux64_v2.3e/gmes/gm_es.pl $GENOME_PATH.masked |tee genemark.log

GeneMark will take some time, about overnight. Note that we used the masked version of our genome. Always use a masked version unless you're using Augustus (for which we will specify the repeat co-ordinates separately).

Another tool (under development) is Gavin Huttley's 'projection' approach. This approach takes a well annotated genome and 'projects' its gene models to your un-annotated genome. We will not use it for this tutorial but see the link:procedure.html#projection[procedure] on how to use it.

== Step 2a: Acquire a 'golden sub-set' of gene models
For phase 2, we assume you have completed the PASA step

We require to identify some gene models that are complete and of very high quality. These can be use downstream to train our de-novo predictors. Traditionally, fewer than 100 genes have been used but this was a limitation of the availability of data. In this part we can identify '1000s' of such golden models but we will only use a subset: some we will keep for validation of the output.

Now we need to create a golden gene set for training. First, PASA has a script that uses TransDecoder to convert the PASA output to CDS-aware genome co-ordinates. As we will show later, there is a significant number of not-so-golden genes in that subset (sensitivity is very low). So with JAMG we can use the 'prepare_golden_genes_for_predictors.pl' to acquire a subset of good gene models informed by a splice aware aligner such as exonerate.

[source,bash]
$JAMG_PATH/3rd_party/PASA/scripts/pasa_asmbls_to_training_set.dbi --pasa_transcripts_fasta ./*.assemblies.fasta \
 --pasa_transcripts_gff3 ./*.pasa_assemblies.gff3
$JAMG_PATH/bin/prepare_golden_genes_for_predictors.pl -genome $GENOME_PATH.masked -softmasked $GENOME_PATH.softmasked \
 -same_species -intron $MAX_INTRON_LENGTH -cpu $LOCAL_CPUS -norefine -complete -no_single \
 -pasa_gff ./*.assemblies.fasta.transdecoder.gff3 \
 -pasa_peptides ./*.assemblies.fasta.transdecoder.pep \
 -pasa_cds ./*.assemblies.fasta.transdecoder.cds \
 -pasa_genome ./*.assemblies.fasta.transdecoder.genome.gff3 \
 -pasa_assembly ./*.assemblies.fasta
ls -l *golden.gff3

Evaluate [todo]
[source,bash]
$JAMG_PATH/3rd_party/PASA/misc_utilities/gff3_to_gtf_format.pl jamg_drosie_tutorial.assemblies.fasta.transdecoder.genome.gff3 $GENOME_PATH > jamg_drosie_tutorial.assemblies.fasta.transdecoder.genome.gtf 
$JAMG_PATH/3rd_party/eval-2.2.8/evaluate_gtf.pl -g dmel-all-no-analysis-r5.53.gff3.gff3.clean.gtf jamg_drosie_tutorial.assemblies.fasta.transdecoder.genome.gtf

== Step 2b: Train and 'run de-novo predictors' that need no evidence
Some predictors like SNAP and GlimmerHMM can use evidence as an option but they (GlimmerHMM at least) takes longer and the results in a small test I did were not as good as without adding additional weights. Regardless, we first need to train them using our golden gene sets from above. 

First, training and running SNAP is very easy now that we have the ZFF files:

[source,bash]
# SNAP - specific instructions
mkdir snap; cd snap
#train
mkdir train ; cd train 
# also copy/link the relevant .fasta .zff data used below
ln -s ../../*zff* ../../*gff3.fasta .
$JAMG_PATH/3rd_party/bin/fathom ./*golden.train.zff ./*golden.train.gff3.fasta -gene-stats | tee gene.statistics.log
$JAMG_PATH/3rd_party/bin/fathom ./*golden.train.zff ./*golden.train.gff3.fasta -categorize 1000
$JAMG_PATH/3rd_party/bin/fathom -export 1000 -plus uni.ann uni.dna
$JAMG_PATH/3rd_party/snap/forge export.ann export.dna
$JAMG_PATH/3rd_party/snap/hmm-assembler.pl $GENOME_NAME . > $GENOME_NAME.hmm # model to use to predict
#predict
cd ../ ; mkdir predict; cd predict
# create a directory where each genome sequence is in a single file. Use the softmasked repeats
ln -s $GENOME_PATH.softmasked $GENOME_NAME.softmasked
$JAMG_PATH/bin/splitfasta.pl -i $GENOME_NAME.softmasked
# prepare execution for each genome sequence
find $GENOME_NAME.softmasked_dir1 -maxdepth 1 -type f -exec sh -c \
 'echo "$JAMG_PATH/3rd_party/snap/snap ../train/$GENOME_NAME.hmm $1 -lcmask -quiet > $1.snap 2>/dev/null ; \
  $JAMG_PATH/3rd_party/evidencemodeler/OtherGeneFinderTrainingGuide/SNAP/SNAP_output_to_gff3.pl $1.snap $1 > $1.snap.gff3 ; \
  $JAMG_PATH/3rd_party/PASA/misc_utilities/gff3_to_gtf_format.pl $1.snap.gff3 $1 > $1.snap.gtf"' \
  find-copy '{}' \; > snap.commands
ParaFly -c snap.commands -CPU $LOCAL_CPUS -v -shuffle
cat $GENOME_NAME.softmasked_dir1/*snap.gtf > snap.gtf
cd ../../

Snap is very fast. For Drosophila, this will take some time (5-10 minutes) because our genome is split in whole chromosomes, i.e. only 15 commands each of which will take considerable time. In most NGS projects you will have thousands (or dozens) of scaffolds so each one will be rather quick. By the way, if you want to use external evidence, please check out the link:procedure.html#snap-external[procedure for SNAP external evidence] documentation.

Next we process GlimmerHMM.

[source,bash]
mkdir -p glimmer/train; cd glimmer/train
ln -s ../../*glimmer* ../../*golden*.fasta .
$JAMG_PATH/3rd_party/GlimmerHMM/train/trainGlimmerHMM \
 ./*.train.gff3.fasta ./*.train.gb \
 -d attempt1 >/dev/null
cd ../


== Step 2c: 'Prepare evidence' for Augustus and train
blah

[source,bash]
todo

== Step 3: 'Run Augustus'
For phase 3, we assume you have completed all of the previous steps (except perhaps running the other de-novo predictors)

[source,bash]
todo

== Step 4: Integrate with EvidenceModeller and add RNA-seq supported UTR
Phase 4 requires all of the previous phases to have been completed.

[source,bash]
todo

== Step 5a: Funcational annotation with JAMp
Phase 5 is required if you are happy with your annotation and now you'd like to manually curate it.

[source,bash]
todo

== Step 5b: Deploy WebApollo
blah


